{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tamil-BERT-Language-Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPN/suaKjTbq5waYZ5fPIH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveengovianalytics/nlp-Tamil/blob/master/Tamil_BERT_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfNzM4GTRrmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlzo70alRtIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs_CAXOaRtLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFjs3OLbRtPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a8b4f68b-5ab1-4de3-e5e2-70923bf64251"
      },
      "source": [
        "! pip install tokenizers==0.4.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers==0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/0a/096a279ee20a206a58ba9af3251c7271fb9493bed3bf29bfe4c58fcbfe4c/tokenizers-0.4.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8r8m6EISPmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7d15a7e2-d494-4b26-fbde-b7245c7d5cc1"
      },
      "source": [
        "!wget https://traces1.inria.fr/oscar/files/Compressed/ta_dedup.txt.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-13 11:25:23--  https://traces1.inria.fr/oscar/files/Compressed/ta_dedup.txt.gz\n",
            "Resolving traces1.inria.fr (traces1.inria.fr)... 128.93.193.43\n",
            "Connecting to traces1.inria.fr (traces1.inria.fr)|128.93.193.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1018374744 (971M) [application/gzip]\n",
            "Saving to: ‘ta_dedup.txt.gz’\n",
            "\n",
            "ta_dedup.txt.gz     100%[===================>] 971.20M  31.6MB/s    in 32s     \n",
            "\n",
            "2020-04-13 11:25:56 (30.5 MB/s) - ‘ta_dedup.txt.gz’ saved [1018374744/1018374744]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYdhkUqkSiwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ta_Data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z-E8EcUS8RR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a8e8c380-5138-4a70-cb70-895937cde7e3"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esperberto-merges.txt  ta_Data\t\t      tamilberto-vocab.json\n",
            "esperberto-vocab.json  ta_dedup.txt\n",
            "sample_data\t       tamilberto-merges.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWgzk967TRX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip ta_dedup.txt.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgEY_PIcUz5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install tokenizers==0.4.2\n",
        "\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = '/content/ta_dedup.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf7DCr1tU5Bk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3eafaa31-afa2-4198-e4e4-f963f4f361b2"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esperberto-merges.txt  ta_Data\t\t      tamilberto-vocab.json\n",
            "esperberto-vocab.json  ta_dedup.txt\n",
            "sample_data\t       tamilberto-merges.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbroY5rjRtRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "# Save files to disk\n",
        "tokenizer.save(\".\", \"tamilberto\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icd3K4TcShIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4886964-317d-4313-9138-00b8ab7203fb"
      },
      "source": [
        "#!cat /content/tamilberto-merges.txt"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#version: 0.2 - Trained by `huggingface/tokenizers`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxTm3r4ISNzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./models/EsperBERTo-small/vocab.json\",\n",
        "    \"./models/EsperBERTo-small/merges.txt\",\n",
        ")\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "print(\n",
        "    tokenizer.encode(\"Mi estas Julien.\")\n",
        ")\n",
        "# Encoding(num_tokens=7, ...)\n",
        "# tokens: ['<s>', 'Mi', 'Ġestas', 'ĠJuli', 'en', '.', '</s>']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}